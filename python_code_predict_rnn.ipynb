{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "python_code_predict_rnn (3).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnn_11MGXVVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "\n",
        "########## License #########################################\n",
        "The code in this Python notebook is licensed under : MIT License\n",
        "This code has been developed for research purposes only.\n",
        "\n",
        "############################################################\n",
        "MIT License\n",
        "\n",
        "Time-series Prediction by using Recurrent Neural Network Model\n",
        "Copyright (c) 2020 Mohamed Hawas\n",
        "\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "############################################################\n",
        "############################################################\n",
        "############################################################\n",
        "\n",
        "Structure of the python notebook:\n",
        "    1- Cell 1 = All used functions - to be run before using the code.\n",
        "    ## The following 4 cells include the code to execute the functions - the recursive train and predict process.\n",
        "    2- Cell 2 = Deterministic mode 1 (30 time-steps 15*X + 15*Y).\n",
        "    3- Cell 3 = Deterministic mode 2 (40 time-steps 20*X + 20*Y).\n",
        "    4- Cell 4 = Non-deterministic mode (30 time-steps 15*X + 15*Y).\n",
        "    5- Cell 5 = Non-deterministic mode - Technical validation model (30 time-steps 15*X + 15*Y).\n",
        "    ##\n",
        "    6- Cell 6 = Code to analyze the results.\n",
        "    7- Cell 7 = Code to quickly test the models.\n",
        "\n",
        "*** note*** the retrain and reconstruct dataset functions are removed from this notebook, \n",
        "            but various parameters that relate to them exist in the code without functionality.\n",
        "\n",
        "############################################################\n",
        "############################################################\n",
        "############################################################\n",
        "############################################################\n",
        "########### License of used libraries ###########\n",
        "\n",
        "- The code uses several open source libraries, the licenses of these libraries are indicated below.\n",
        "\n",
        "########### License of Numpy ###########\n",
        "The code uses Numpy library that uses BSD 3-Clause \"New\" or \"Revised\" License\n",
        "Copyright (c) 2005-2020, NumPy Developers.\n",
        "All rights reserved.\n",
        "\n",
        "Redistribution and use in source and binary forms, with or without\n",
        "modification, are permitted provided that the following conditions are\n",
        "met:\n",
        "\n",
        "    * Redistributions of source code must retain the above copyright\n",
        "       notice, this list of conditions and the following disclaimer.\n",
        "\n",
        "    * Redistributions in binary form must reproduce the above\n",
        "       copyright notice, this list of conditions and the following\n",
        "       disclaimer in the documentation and/or other materials provided\n",
        "       with the distribution.\n",
        "\n",
        "    * Neither the name of the NumPy Developers nor the names of any\n",
        "       contributors may be used to endorse or promote products derived\n",
        "       from this software without specific prior written permission.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n",
        "\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n",
        "LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n",
        "A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n",
        "OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n",
        "SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n",
        "LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n",
        "DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n",
        "THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        "(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "\n",
        "########### License of Pandas ###########\n",
        "The code uses Pandas library that uses BSD 3-Clause License\n",
        "\n",
        "BSD 3-Clause License\n",
        "\n",
        "Copyright (c) 2008-2011, AQR Capital Management, LLC, Lambda Foundry, Inc. and PyData Development Team\n",
        "All rights reserved.\n",
        "\n",
        "Copyright (c) 2011-2020, Open source contributors.\n",
        "\n",
        "Redistribution and use in source and binary forms, with or without\n",
        "modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "* Redistributions of source code must retain the above copyright notice, this\n",
        "  list of conditions and the following disclaimer.\n",
        "\n",
        "* Redistributions in binary form must reproduce the above copyright notice,\n",
        "  this list of conditions and the following disclaimer in the documentation\n",
        "  and/or other materials provided with the distribution.\n",
        "\n",
        "* Neither the name of the copyright holder nor the names of its\n",
        "  contributors may be used to endorse or promote products derived from\n",
        "  this software without specific prior written permission.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
        "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
        "FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
        "OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "\n",
        "########### License of Keras ###########\n",
        "The MIT License (MIT)\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "\n",
        "########### License of Tensorflow ###########\n",
        "\n",
        "tensorflow is licensed under the Apache License 2.0\n",
        "Copyright 2019 The TensorFlow Authors.  All rights reserved.\n",
        "Apache License\n",
        "Version 2.0, January 2004\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "########### License of nltk ###########\n",
        "Apache License Version 2.0\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "########### License of tqdm ###########\n",
        "Mozilla Public Licence (MPL) v. 2.0 - Exhibit A\n",
        "This Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0. \n",
        "If a copy of the MPL was not distributed with this file, You can obtain one at https://mozilla.org/MPL/2.0/.\n",
        "\n",
        "MIT License (MIT)\n",
        "Copyright (c) 2013 noamraph\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this \n",
        "software and associated documentation files (the \"Software\"), to deal in the Software \n",
        "without restriction, including without limitation the rights to use, copy, modify, merge, \n",
        "publish, distribute, sublicense, and/or sell copies of the Software, and to permit \n",
        "persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "The above copyright notice and this permission notice shall be included in all \n",
        "copies or substantial portions of the Software.\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, \n",
        "INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR \n",
        "PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE \n",
        "FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n",
        "TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE\n",
        "OR OTHER DEALINGS IN THE SOFTWARE.\n",
        "\n",
        "########### License of matplotlib ###########\n",
        "BSD\n",
        "https://matplotlib.org/users/license.html\n",
        "\n",
        "########### License of sklearn ###########\n",
        "New BSD License\n",
        "\n",
        "Copyright (c) 2007–2020 The scikit-learn developers.\n",
        "All rights reserved.\n",
        "\n",
        "\n",
        "Redistribution and use in source and binary forms, with or without\n",
        "modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "  a. Redistributions of source code must retain the above copyright notice,\n",
        "     this list of conditions and the following disclaimer.\n",
        "  b. Redistributions in binary form must reproduce the above copyright\n",
        "     notice, this list of conditions and the following disclaimer in the\n",
        "     documentation and/or other materials provided with the distribution.\n",
        "  c. Neither the name of the Scikit-learn Developers  nor the names of\n",
        "     its contributors may be used to endorse or promote products\n",
        "     derived from this software without specific prior written\n",
        "     permission. \n",
        "\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
        "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
        "ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR\n",
        "ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n",
        "LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n",
        "OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\n",
        "DAMAGE.\n",
        "\n",
        "########### License of xgboost ###########\n",
        "\n",
        "   Copyright (c) 2019 by Contributors\n",
        "\n",
        "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "   you may not use this file except in compliance with the License.\n",
        "   You may obtain a copy of the License at\n",
        "\n",
        "       http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "   Unless required by applicable law or agreed to in writing, software\n",
        "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "   See the License for the specific language governing permissions and\n",
        "   limitations under the License.\n",
        "\n",
        "########### License of joblib ###########\n",
        "BSD license\n",
        "https://joblib.readthedocs.io/en/latest/\n",
        "\n",
        "############################################################\n",
        "############################################################\n",
        "''' \n",
        "\n",
        "# general imports\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk import flatten\n",
        "import copy\n",
        "from tensorflow.python.keras.layers import RepeatVector, Multiply, merge\n",
        "from tensorflow.python.keras.layers import TimeDistributed, Input\n",
        "from sklearn.model_selection import RepeatedKFold, train_test_split\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.python.keras.layers import Activation, Dense, Dropout, BatchNormalization, LeakyReLU, Conv1D, Conv2D, Conv3D, Flatten, MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.models import load_model, Model\n",
        "import pandas as pd\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adadelta, Nadam, Adagrad\n",
        "from keras import regularizers\n",
        "from tensorflow.python.keras.layers import Embedding, LSTM, Bidirectional, RNN, GRU, ConvLSTM2D\n",
        "from array import array\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer, RobustScaler, LabelEncoder\n",
        "pd.set_option('display.max_rows', 20)\n",
        "\n",
        "def gen_data(data_tmp=None, time_series=True, days=None, target_pred=None, tr_data_dir=None, crop_point=110):\n",
        "    '''this function is used to extract time-series data from a data source '''\n",
        "    n_data = []; dataset_tmp = []; c = 0\n",
        "    for i in list(data_tmp):\n",
        "        nex = 0; new_i = []; new_i.extend(list(i[:4]))\n",
        "        mean = []\n",
        "        for ii in i[4:]:\n",
        "            mean.append(ii)\n",
        "            if nex == 0: daily_increase = ii; prev = 0; prev_daily = 0\n",
        "            else: daily_increase = ii - prev; \n",
        "            new_i.append(daily_increase); prev = ii; nex += 1\n",
        "        n_data.append(new_i)\n",
        "\n",
        "    selected_target_pred = target_pred\n",
        "    dataset_tmp = pd.DataFrame(n_data)\n",
        "    target_pred_data = dataset_tmp[dataset_tmp[1] == (selected_target_pred)]\n",
        "    target_pred_data.to_csv(tr_data_dir+'/'+selected_target_pred+'_data-all.csv', index=False); \n",
        "    target_pred_data_all = target_pred_data\n",
        "    data_v  = target_pred_data.columns.values\n",
        "    target_pred_data = target_pred_data[data_v[4:crop_point]]\n",
        "    target_pred_data.to_csv(tr_data_dir+'/'+selected_target_pred+'_data.csv', index=False)\n",
        "    dataset_tmp = dataset_tmp[data_v[4:crop_point]]\n",
        "    dataset_tmp.to_csv(tr_data_dir+'/'+'data.csv', index=False)\n",
        "    return dataset_tmp, target_pred_data, target_pred_data_all\n",
        "        \n",
        "def gen_rnn_data(link=None, time_steps=None, retrain=False, scaler=False, crop_point=110,\n",
        "                 gen_time_series=True, target_pred=None, tr_data_dir=None):\n",
        "    '''this function is used to generate time-series data for training the models and prediction processes'''\n",
        "\n",
        "    print('- Generating data for time-series training and prediction...')\n",
        "    data = pd.read_csv(link).to_numpy()\n",
        "    data, target_pred_cropped, target_pred_uncropped = gen_data(data_tmp=data, time_series=True, crop_point=crop_point,\n",
        "                                                        target_pred=target_pred, tr_data_dir=tr_data_dir)\n",
        "    data = data.to_numpy(); target_pred_cropped = (target_pred_cropped.to_numpy()[0]); \n",
        "    target_pred_uncropped = target_pred_uncropped.drop([0,1,2,3], axis=1).to_numpy()[0]\n",
        "    \n",
        "    ndata = []\n",
        "    for i in data: ndata.append(list(i))\n",
        "    target_pred_index = ndata.index(list(target_pred_cropped))\n",
        "    print('    - Origianl dataset shape: ', data.shape)\n",
        "\n",
        "    # scale data\n",
        "    if scaler == True:\n",
        "        scaler = MinMaxScaler(feature_range=(0,1)).fit(data)\n",
        "        data = scaler.transform(data); target_pred_cropped = scaler.transform(target_pred_cropped.reshape(1,-1))\n",
        "    else: scaler = 0\n",
        "\n",
        "    # divide into X, Y\n",
        "    if (isinstance(retrain, bool) == True):\n",
        "        start = int(data.shape[1] - (time_steps*2)); split = start + time_steps\n",
        "        X, Y = data[:,start:split], data[:,split:]\n",
        "    else: \n",
        "        start = int(retrain.shape[1] - (time_steps*2)); split = start + time_steps\n",
        "        X, Y = retrain[:,start:split], retrain[:,split:];  \n",
        "        target_pred_cropped = retrain[eg_index]; data = retrain\n",
        "    X = (X).reshape(X.shape[0], time_steps, 1)\n",
        "    Y = (Y).reshape(Y.shape[0], time_steps, 1)\n",
        "    data = data.reshape(data.shape[0], data.shape[1], 1)\n",
        "    input_shape = (time_steps, 1)\n",
        "    print('    - X shape: ', X.shape, ' / Y shape: ', Y.shape)\n",
        "\n",
        "    # generate time-series data\n",
        "    if gen_time_series == True:\n",
        "        gen_xx = []; gen_yy = []; c = 0\n",
        "        for dxy in data[:, time_steps*3:]:\n",
        "            for g_loop in range((time_steps*1)+1):\n",
        "                gen_xx.append(dxy[g_loop:time_steps+g_loop]); \n",
        "                gen_yy.append(dxy[time_steps+g_loop:(time_steps*2)+g_loop])\n",
        "            c+=1\n",
        "        gen_xx, gen_yy = np.array(gen_xx), np.array(gen_yy); \n",
        "        gen_xx = gen_xx.reshape(gen_xx.shape[0], time_steps, 1); print(gen_xx.shape)\n",
        "        gen_yy = gen_yy.reshape(gen_yy.shape[0], time_steps, 1); print(gen_yy.shape)\n",
        "        input_shape = (time_steps, 1)\n",
        "    else: gen_xx, gen_yy = 0, 0\n",
        "\n",
        "    # test on target_pred's data only\n",
        "    cx = X[target_pred_index]; cy = Y[target_pred_index]; \n",
        "    cx = cx.reshape(cx.shape[1], time_steps, 1); cy = cy.reshape(cy.shape[1], time_steps, 1); \n",
        "    return start, split, target_pred_cropped.reshape(-1,1), target_pred_uncropped, X, Y, \\\n",
        "    cx, cy, gen_xx, gen_yy, scaler, input_shape, data\n",
        "\n",
        "def train_rnn(X=None, Y=None, time_steps=None, epochs=300, bs=32, vl=0.1, vd=None, retrain=False,\n",
        "              dropout=0, rnn=None, convrnn=False, l=[64, 100, 32], input_shape=None, \n",
        "              tdm_dir=None, rtdm_dir=None, set_dir=None, functional_api=True, seed_input_tr=0):\n",
        "    '''this function is used to train the models'''\n",
        "    import uuid \n",
        "    import os\n",
        "    import random\n",
        "    import joblib\n",
        "\n",
        "    if rnn != None:\n",
        "        idx = str(uuid.uuid4()) \n",
        "        if retrain == False: mod_s = 'model_trained_'+idx+'.h5'\n",
        "        else: mod_s = 'model_retrained_'+idx+'.h5'\n",
        "\n",
        "        settings_rnn = {'model': mod_s, 'time-steps':time_steps, 'epochs':epochs, 'batch-size':bs, \n",
        "                        'validation-split':vl, 'rnn':str(rnn)[39:-2], 'layers':l, 'dropout':dropout, \n",
        "                        'conv-rnn':convrnn, 'seed-python':seed_input_tr[0], \n",
        "                        'seed-tf':seed_input_tr[1], 'functional-api':functional_api}\n",
        "        pass_training = False\n",
        "        try: \n",
        "            settings_dict = joblib.load(set_dir+'/settings_dict.pkl')\n",
        "            #\n",
        "            new_settings = {k:v for k, v in settings_rnn.items() if list(settings_rnn).index(k)!= 0}\n",
        "            for kk, vv in settings_dict.items():\n",
        "                saved_settings = {k:v for k, v in vv.items() if list(vv).index(k)!= 0}\n",
        "                if new_settings == saved_settings: pass_training = True; break\n",
        "            #\n",
        "        except: settings_dict = {}\n",
        "\n",
        "        if pass_training == False:\n",
        "            if functional_api == False:\n",
        "                model = Sequential()\n",
        "                # encoder layer\n",
        "                if convrnn == True:\n",
        "                    model.add(Conv1D(filters=l[0], kernel_size=3, activation='elu', input_shape=input_shape))\n",
        "                    if dropout != 0: model.add(Dropout(dropout))\n",
        "                    model.add(MaxPooling1D(pool_size=2))\n",
        "                    model.add(Conv1D(filters=int(l[0]/2), kernel_size=3, activation='elu'))\n",
        "                    if dropout != 0: model.add(Dropout(dropout))\n",
        "                    model.add(MaxPooling1D(pool_size=2))\n",
        "                    model.add(Flatten())\n",
        "                    model.add(RepeatVector(time_steps))\n",
        "                else:\n",
        "                    model.add((rnn(l[0], activation='elu', input_shape=input_shape))) #64\n",
        "                    if dropout != 0: model.add(Dropout(dropout))\n",
        "                    model.add(RepeatVector(time_steps))\n",
        "                    \n",
        "                # decoder layer\n",
        "                model.add((rnn(l[1], activation='elu', return_sequences=True))) #100\n",
        "                if dropout != 0: model.add(Dropout(dropout))\n",
        "                if len(l) > 2: \n",
        "                    model.add(rnn(l[2], activation='elu', return_sequences=True)) #32\n",
        "                    if dropout != 0: model.add(Dropout(dropout))\n",
        "                model.add(TimeDistributed(Dense(1, activation=None)))\n",
        "\n",
        "            else:\n",
        "                visible = Input(shape=input_shape)\n",
        "                # encoder layer\n",
        "                if convrnn == True:\n",
        "                    encoder = (Conv1D(filters=l[0], kernel_size=3, activation='elu'))(visible)\n",
        "                    if dropout != 0: (Dropout(dropout))(encoder)\n",
        "                    encoder = (MaxPooling1D(pool_size=2))(encoder)\n",
        "                    encoder = (Flatten())(encoder)\n",
        "                else:\n",
        "                    encoder = ((rnn(l[0], activation='elu')))(visible)\n",
        "                    if dropout != 0: encoder = (Dropout(dropout))(encoder)\n",
        "\n",
        "                # reconstruct layer\n",
        "                decoder1 = (RepeatVector(time_steps))(encoder)\n",
        "                decoder1 = ((rnn(l[1], activation='elu', return_sequences=True)))(decoder1)\n",
        "                if dropout != 0: decoder1 = (Dropout(dropout))(decoder1)\n",
        "                decoder1 = ((Dense(1, activation='linear')))(decoder1)\n",
        "\n",
        "                # predict decoder\n",
        "                decoder2 = (RepeatVector(time_steps))(encoder)\n",
        "                decoder2 = ((rnn(l[2], activation='elu', return_sequences=True)))(decoder2)\n",
        "                if dropout != 0: decoder2 = (Dropout(dropout))(decoder2)\n",
        "                decoder2 = ((Dense(1, activation='linear')))(decoder2)            \n",
        "                \n",
        "                #connect decoders\n",
        "                model = Model(inputs=visible, outputs=[decoder1, decoder2])\n",
        "            \n",
        "            optimizer = Adam(learning_rate=0.001, beta_1=0.8, beta_2=0.82, amsgrad=False) #b2: 82\n",
        "            optimizer = Adadelta(learning_rate=1, rho=0.85)\n",
        "            optimizer = RMSprop(learning_rate=0.001, rho=0.85)\n",
        "            # model.build((1,time_steps, 1))\n",
        "            model.compile(optimizer=optimizer, loss='mae', metrics='accuracy')\n",
        "            print(model.summary())\n",
        "            \n",
        "            history = model.fit(X, Y, epochs=epochs, validation_data=vd, validation_split= vl,\n",
        "                                verbose=3, batch_size=bs, shuffle=True)\n",
        "            if retrain == False: model.save(tdm_dir+'/model_trained_'+idx+'.h5')\n",
        "            else: model.save(rtdm_dir+'/model_retrained_'+idx+'.h5')\n",
        "            settings_dict[mod_s] = settings_rnn\n",
        "            joblib.dump(settings_dict, set_dir+'/settings_dict.pkl')\n",
        "            return model, history, settings_dict\n",
        "        else: return None, None, None\n",
        "    else: return None, None, None\n",
        "\n",
        "def plot_predictions(prediction=None, time_steps=None, extend_pr=None, mod_name=None, thrsh=None, \n",
        "                     rlist=None, eval_pr=None, sn=None, ext='2020-10-01', eval_acc=None, graphs_dir=None, \n",
        "                     save_accuracy=0.7):\n",
        "    '''this function is used to plot the predictions'''\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Get values\n",
        "    y_predicted_ts = (prediction[1][time_steps*2: time_steps*3])#+3 \n",
        "    y_predicted_eval_pr = y_predicted_ts[eval_pr-1:eval_pr]\n",
        "    y_predicted_all_loc = prediction[1][len(prediction[3].dropna())-1:len(prediction[3].dropna())]\n",
        "    y_predicted_all = prediction[1][time_steps*2:len(prediction[3].dropna())] #+eval_pr instead of *2\n",
        "    y_true_data = (prediction[3][:])\n",
        "    y_true_data_end = (prediction[3][len(prediction[3].dropna())-1:])\n",
        "    y_used_data_start = (prediction[3][0:1])\n",
        "    y_used_data_end = (prediction[3][30:31])\n",
        "    to_date = prediction.index[prediction[0] == ext].tolist()[0] + 1\n",
        "    y_predicted_plus = prediction[1][len(prediction[3].dropna())-1:to_date]\n",
        "    y_predicted_all_evaluated = prediction[2][time_steps*2:to_date]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12,8))\n",
        "    plt.plot(y_predicted_all, label='prediction to evaluation end-point - total duration acc: %.2f' % (rlist[2]*100)+' %', \n",
        "             color='black', linestyle='solid', marker='o', markersize=3, linewidth=0.7)\n",
        "    plt.plot(y_predicted_all_evaluated, label='evaluated prediction (trng acc=%.2f' % (eval_acc)+') - total duration acc: %.2f' % (rlist[3]*100)+' %', \n",
        "             color='red', linestyle='solid', marker='o', markersize=3, linewidth=0.7)\n",
        "    plt.plot(y_used_data_start, label='used training data - start-point', \n",
        "             color='orange', linestyle='solid', marker='o', markersize=10)\n",
        "    plt.plot(y_used_data_end, label='used training data - end-point', \n",
        "             color='orange', linestyle='solid', marker='o', markersize=10)\n",
        "    plt.plot(y_predicted_eval_pr, label='prediction - to threshold point acc: %.2f' % (rlist[0]*100)+' %', \n",
        "             color='yellow', linestyle='solid', marker='o', markersize=10)\n",
        "    plt.plot(y_predicted_all_loc, label='prediction - total duration end-point (evaluation end-point)', \n",
        "             color='black', linestyle='solid', marker='o', markersize=10)\n",
        "    plt.plot(y_predicted_ts, label='prediction - time_steps acc: %.2f' % (rlist[1]*100)+' %', \n",
        "             color='blue', linestyle='solid', marker='o', markersize=5)\n",
        "    plt.plot(y_true_data, label='True value numbers', color='green', linestyle='dashed')\n",
        "    plt.plot(y_true_data_end, label='True value numbers - end-point', \n",
        "             color='green', linestyle='solid', marker='o', markersize=10)\n",
        "    plt.plot(y_predicted_plus, label='prediction to: '+ext, \n",
        "             color='purple', linestyle='dashed')\n",
        "\n",
        "    plt.legend(bbox_to_anchor=(0,1.08,1,0.2), loc=\"lower left\")\n",
        "    \n",
        "    if thrsh == 0: thrsh = 100\n",
        "    else: thrsh = thrsh*10\n",
        "    thrsh_title = 'Predicting with threshold of ' + str(thrsh) + ' % of time-step(s)'\n",
        "    plt.title('Trend of '+mod_name+'\\n' + thrsh_title)\n",
        "    plt.ylabel('value'); plt.xlabel('days')\n",
        "    gext = '.pdf'\n",
        "    if os.path.exists(graphs_dir+'/graph_'+sn+'_'+mod_name+gext) == True: \n",
        "        rndn = str(np.random.uniform(0,5))\n",
        "        fd = graphs_dir+'/graph_'+sn+'_'+mod_name+gext\n",
        "    else: fd = graphs_dir+'/graph_'+sn+'_'+mod_name+gext; rndn = None\n",
        "    if rlist[2] > save_accuracy:\n",
        "        plt.gcf().savefig(fd, bbox_inches='tight', dpi=300)\n",
        "    plt.close()\n",
        "      \n",
        "    return fd\n",
        "\n",
        "def eval_predictions(prediction=None, time_steps=None, eval_pr=None):\n",
        "    '''this function is used to evaluate the predictions'''\n",
        "    from sklearn.metrics import mean_absolute_error\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    from sklearn.metrics import r2_score\n",
        "\n",
        "    t = time_steps\n",
        "\n",
        "    y_predicted = (prediction[1][time_steps+t: time_steps*3])#+3\n",
        "    y_true = (prediction[3][time_steps+t: time_steps*3])#+3\n",
        "\n",
        "    try: r2 = r2_score(y_true[:eval_pr], y_predicted[:eval_pr])\n",
        "    except: r2 = 0\n",
        "    try: r2_ts = r2_score(y_true[:time_steps], y_predicted[:time_steps])\n",
        "    except: r2_ts = 0\n",
        "    try: \n",
        "        r2_all = r2_score(prediction[3][time_steps+t:len(prediction[3].dropna())], \n",
        "                        prediction[1][time_steps+t:len(prediction[3].dropna())])\n",
        "        r2_all_eval = r2_score(prediction[3][time_steps+t:len(prediction[3].dropna())], \n",
        "                        prediction[2][time_steps+t:len(prediction[3].dropna())])\n",
        "    except: r2_all = 0;  r2_all_eval = 0\n",
        "    del prediction\n",
        "    return r2, r2_ts, r2_all, [r2, r2_ts, r2_all, r2_all_eval]\n",
        "\n",
        "def predict(model=None, loop_input=None, target_pred_cropped=None, target_pred_uncropped=None, sn='nan', \n",
        "            time_steps=None, loops=1, scaler=False, thrsh=1, start=None, split=None, save_prd=True,\n",
        "            eval_pr=None, reconst=False, acc=0.85, ext='2020-10-01', save_accuracy=0.7, \n",
        "            use_dir=None, set_dir=None, pred_dir=None, graphs_dir=None):\n",
        "    '''\n",
        "    this function is used to predict\n",
        "    loops = the loops of recursive prediction process\n",
        "    '''\n",
        "    from datetime import date, timedelta\n",
        "    from tqdm import tqdm\n",
        "    from tensorflow.python.keras.models import load_model\n",
        "    import os\n",
        "    import contextlib\n",
        "    import joblib\n",
        "    pd.set_option('display.max_rows', None)\n",
        "\n",
        "    mod_n = str(thrsh)+'_'+model[len(use_dir)+1:]\n",
        "    model = load_model(model)\n",
        "    input_reshape = (1, time_steps, 1)\n",
        "    loop_input = loop_input.reshape(input_reshape)\n",
        "    if loops == 'auto':\n",
        "        from_loop = date(2020, 1, 22)+timedelta(days=start+time_steps*2)\n",
        "        try: to_loop = date(int(ext[:4]), int(ext[5:7]), int(ext[8:10]))\n",
        "        except: to_loop = date(int(ext[:4]), int(ext[5:7]), int(ext[9]))\n",
        "        diff = list(str(to_loop-from_loop)[:8])\n",
        "        loops = int(''.join([dt for dt in diff if dt.isdigit()]))\n",
        "        if thrsh == 0: loops = int(loops / time_steps) + 1\n",
        "        else: loops = loops = int(loops / thrsh) + 1\n",
        "        # print(loops)\n",
        "    for i in range(loops):\n",
        "        loop_output = model.predict_on_batch(loop_input)\n",
        "        if isinstance(scaler, bool) == False: \n",
        "            loop_output = np.concatenate(((target_pred_cropped)[:split].reshape(1,split,1), loop_output[0]), axis=1)\n",
        "            loop_output = scaler.inverse_transform(loop_output.reshape(1,target_pred_cropped.shape[0]))[:,split:]\n",
        "        try: loop_output = np.ndarray.flatten(loop_output);\n",
        "        except: \n",
        "            loop_output = np.ndarray.flatten(loop_output[1])\n",
        "        if i == 0: \n",
        "            if thrsh == 0: \n",
        "                pred_table = loop_output; \n",
        "                loop_input = loop_output.reshape(input_reshape)\n",
        "            else: \n",
        "                pred_table = loop_output.reshape(input_reshape)[:,0:thrsh]\n",
        "                loop_input = np.concatenate((loop_input[:,thrsh:], \n",
        "                                             loop_output.reshape(input_reshape)[:,0:thrsh]), axis=1)\n",
        "        else: \n",
        "            if thrsh == 0: \n",
        "                pred_table = np.concatenate((pred_table, loop_output), axis=0); \n",
        "                loop_input = loop_output.reshape(input_reshape)\n",
        "            else:\n",
        "                pred_table = np.concatenate((pred_table, loop_output.reshape(input_reshape)[:,0:thrsh]), axis=0); \n",
        "                loop_input = np.concatenate((loop_input[:,thrsh:], \n",
        "                                             loop_output.reshape(input_reshape)[:,0:thrsh]), axis=1)\n",
        "    if thrsh == 0: thrsh0 = 1\n",
        "    else: thrsh0 = thrsh\n",
        "    pred_table = pred_table.reshape(thrsh0*pred_table.shape[0],1)\n",
        "    delta_x = timedelta(days=start); delta_y = timedelta(days=time_steps*(loops+2)-1)\n",
        "    dates_range = []; interval = timedelta(days=1)\n",
        "    start_date = date(2020, 1, 22) + delta_x; end_date = start_date + delta_y\n",
        "    while start_date <= end_date: dates_range.append(start_date.strftime(\"%Y-%m-%d\")); start_date += interval\n",
        "    prediction = pd.concat((pd.DataFrame(target_pred_cropped[start:]), \n",
        "                            pd.DataFrame(pred_table, dtype=int)), axis=0).reset_index(drop=True)\n",
        "    if reconst == False:\n",
        "        prediction = pd.concat((pd.DataFrame(dates_range), \n",
        "                                prediction.rename(columns={0: 1}), \n",
        "                                pd.DataFrame(target_pred_uncropped[start:]).rename(columns={0: 2})), axis=1)\n",
        "        prediction = prediction.reset_index(drop=True)\n",
        "        # Adversarial evaluation\n",
        "        with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
        "            prediction, true_acc = adversarial_evaluation(prediction=prediction, acc=acc)\n",
        "            pred_eval = prediction.rename(columns={'date':0, 'model prediction':1, 'evaluated prediction (acc:%.2f' % (true_acc)+')':2, 'true values':3})\n",
        "        ########################\n",
        "        r2, r2_ts, r2_all, rlist = eval_predictions(prediction=pred_eval, time_steps=time_steps,\n",
        "                                                            eval_pr=eval_pr)\n",
        "        graph_path = plot_predictions(prediction=pred_eval, time_steps=time_steps, extend_pr=10, \n",
        "                                        mod_name=mod_n, thrsh=thrsh, rlist=rlist, eval_pr=eval_pr, \n",
        "                                        sn=sn, ext=ext, eval_acc=true_acc, graphs_dir=graphs_dir, \n",
        "                                        save_accuracy=save_accuracy)\n",
        "    else: \n",
        "        r2, r2_ts, r2_all, graph_path, pred_path = 0, 0, 0, 0, 0\n",
        "        prediction = prediction.reset_index(drop=True)\n",
        "    pred_path = pred_dir+'/prediction_'+mod_n+'_to_'+ext+'.csv'\n",
        "    if os.path.exists(pred_dir) == False: os.makedirs(pred_dir)\n",
        "    to_date = prediction.index[prediction['date'] == '2020-10-01'].tolist()[0] + 1\n",
        "    if (save_prd==True) and (r2_all > save_accuracy): prediction[:to_date].to_csv(pred_path)\n",
        "    del pred_eval; del pred_table; del loop_input; del loop_output\n",
        "    return prediction, r2, r2_ts, r2_all, graph_path, pred_path\n",
        "\n",
        "class block_pr:\n",
        "    '''\n",
        "    Class content removed but it will not affect the functionality of the overall code\n",
        "     '''\n",
        "    def __init__(self, block):\n",
        "        self.block = block\n",
        "    def __enter__(self):\n",
        "        self.block = True\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        self.block = True\n",
        "\n",
        "def save_tables(save_settings=None, ext=10, dates_info=False, prediction_inf=None, \n",
        "                time_steps=None, train_start_date=None, train_end_date=None, set_dir=None):\n",
        "    import pandas as pd\n",
        "    from tqdm import tqdm\n",
        "    \n",
        "    if dates_info == False:\n",
        "        def get_table(save_settings=None, mod_index=0):\n",
        "            for mod_settings in tqdm(save_settings):\n",
        "                mod_settings = [item for item in mod_settings.items()]\n",
        "                mod_settings = dict(mod_settings)\n",
        "                if mod_index == 0: ms_all = pd.DataFrame.from_dict(mod_settings, orient='index').T\n",
        "                else: \n",
        "                    ms = pd.DataFrame.from_dict(mod_settings, orient='index').T\n",
        "                    ms_all = pd.concat((ms_all, ms), axis=0, ignore_index=True)\n",
        "                mod_index += 1\n",
        "            return ms_all\n",
        "        ms_all = get_table(save_settings=save_settings, mod_index=0)\n",
        "        ext_ = 'to_' + str(ext) + '_'\n",
        "        cols = ['r2_all_duration']  + [col for col in ms_all if col != 'r2_all_duration']\n",
        "        ms_all = ms_all[[cols[1]]+[cols[0]]+cols[2:]]\n",
        "        ms_all = (ms_all.sort_values(by=['r2_all_duration'], ascending=False))\n",
        "        ms_all_2col = ms_all[[cols[1]]+[cols[0]]].head(10)\n",
        "        best_mod = ms_all.head(1).T\n",
        "        num_models = ms_all.drop_duplicates(subset='model').shape[0]\n",
        "        num_preds = ms_all.shape[0]; num_graphs = ms_all.shape[0]\n",
        "        inf = pd.DataFrame([['Models', num_models], ['Predictions',num_preds], \n",
        "                        ['Graphs',num_graphs]], columns=['Items', 'Values'])\n",
        "        tables = [ms_all, ms_all_2col, best_mod, inf]; ids = ['complete_', 'brief_', 'best_model_', 'inf_']\n",
        "        for save_loop, idx in zip(tables, ids):\n",
        "            save_loop.to_csv(set_dir+'/metadata/models_accuracy_settings_'+idx+ext_+'.csv')\n",
        "            save_loop.to_html(set_dir+'/metadata/models_accuracy_settings_'+idx+ext_+'.html')\n",
        "    else:\n",
        "        if isinstance(prediction_inf, bool) == False:\n",
        "            ext_ = 'to_' + str(ext)\n",
        "            dates_info_file = 'settings_dates_info_'+ext_+'.csv'\n",
        "            cn = prediction_inf.columns.tolist()\n",
        "            # if dates_info_file not in os.listdir(set_dir+'/metadata'):\n",
        "            y_used_data_start_date = (prediction_inf[cn[0]][0:1])[0]\n",
        "            y_used_data_end_date = list(prediction_inf[cn[0]][(time_steps*2)-1:(time_steps*2)])[0]\n",
        "            y_eval_data_start_date = list(prediction_inf[cn[0]][(time_steps*2):(time_steps*2)+1])[0]\n",
        "            y_eval_data_end_date = list(prediction_inf[cn[0]][len(prediction_inf[cn[3]].dropna())-1:])[0]\n",
        "            inf = pd.DataFrame([['Start date for training data', y_used_data_start_date], \n",
        "                                ['End date for training data', y_used_data_end_date], \n",
        "                                ['Start date for evaluation data', y_eval_data_start_date], \n",
        "                                ['End date for evaluation data', y_eval_data_end_date], \n",
        "                                ['Duration of evaluation data', len(prediction_inf[cn[3]].dropna()) - (time_steps*2)],\n",
        "                                [\"Start date for training process\", train_start_date],\n",
        "                                [\"End date for training process\", train_end_date]], \n",
        "                                columns=['Items', 'Values'])\n",
        "            inf.to_csv(set_dir+'/metadata/'+dates_info_file)\n",
        "            inf.to_html(set_dir+'/metadata/'+dates_info_file[:-3]+'html')\n",
        "        else: pass\n",
        "    if dates_info == False: \n",
        "        try: return ms_all.drop_duplicates(subset='model')['model'].tolist()\n",
        "        except: return []\n",
        "\n",
        "def get_settings(mod=None, settings=None, json_name=None, update_dict=None, get_settings_only=False):\n",
        "    import json\n",
        "    # import ujson\n",
        "    if get_settings_only == False:\n",
        "        if isinstance(settings, dict) == True: \n",
        "            try: \n",
        "                mod_settings = settings[mod]\n",
        "                mod_settings.update(update_dict)\n",
        "            except: \n",
        "                try: \n",
        "                    with open(json_name) as fn: mod_settings = json.load(fn)\n",
        "                    mod_settings.update(update_dict)\n",
        "                except: mod_settings = {}\n",
        "        else: \n",
        "            try: \n",
        "                with open(json_name) as fn: mod_settings = json.load(fn)\n",
        "                mod_settings.update(update_dict)\n",
        "            except: mod_settings = {}\n",
        "    else: \n",
        "        with open(json_name) as fn: mod_settings = json.load(fn)\n",
        "    return mod_settings\n",
        "\n",
        "def save_data_structure(selected_models=None, ext=None, graphs_dir=None, set_dir=None, pred_dir=None, use_dir=None):\n",
        "    import os\n",
        "    import shutil\n",
        "    import pandas as pd\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    md = 'to_'+ext\n",
        "    if os.path.exists(use_dir+'_filtered') == False: os.makedirs(use_dir+'_filtered')\n",
        "    if os.path.exists(set_dir+'_filtered') == False: os.makedirs(set_dir+'_filtered')\n",
        "\n",
        "    for tr_mod in tqdm(selected_models):\n",
        "        shutil.copy2(use_dir+'/'+tr_mod, use_dir+'_filtered'+'/'+tr_mod)\n",
        "    for pred_settings in tqdm(os.listdir(pred_dir)):\n",
        "        path_tmp = 'settings'+pred_settings[10:-18]+'.json'\n",
        "        shutil.copy2(set_dir+'/'+path_tmp, set_dir+'_filtered'+'/'+path_tmp)\n",
        "    shutil.copytree(set_dir+'/metadata', set_dir+'_filtered/metadata')\n",
        "\n",
        "def select_models_acc(evals=None, save_accuracy=None, set_dir=None, mod=None, save_settings=None):\n",
        "    import json\n",
        "\n",
        "    for eval_v in evals:\n",
        "        if eval_v > save_accuracy: \n",
        "            with open(set_dir+'/settings_'+str(evals.index(eval_v))+'_'+mod+'.json') as fn: \n",
        "                add_mod = json.load(fn)\n",
        "            save_settings.append(add_mod)\n",
        "    return save_settings\n",
        "\n",
        "def select_best_models_acc(evals=None, mod=None, set_dir=None, best_model=None):\n",
        "    import json\n",
        "\n",
        "    with open(set_dir+'/settings_'+str(evals.index(max(evals)))+'_'+mod+'.json') as fn: \n",
        "        best_settings = json.load(fn)\n",
        "    # add best settings to best model list for later selection after all loops are done\n",
        "    best_model.append(best_settings)\n",
        "    return best_model, best_settings\n",
        "\n",
        "def predict_select_model(link=None, gen_time_series=None, target_pred=None, tr_data_dir=None, mixed_ts=False,\n",
        "                         loop_input=None, use_dir=None, set_dir=None, bm_dir=None, settings=None, save_accuracy=0.7,\n",
        "                         rnn=None, eval_pr=None, target_pred_cropped=None, ts=None, ext=10, block_printing=False,\n",
        "                         target_pred_uncropped=None, loops=None, t2=None, scaler=False, start=None, split=None, \n",
        "                         train_end_date=None, train_start_date=None, graphs_dir=None, pred_dir=None, crop_point=110):\n",
        "    '''this function is used to predict and select best model'''\n",
        "    from tqdm import tqdm\n",
        "    import joblib\n",
        "    import time\n",
        "    import random\n",
        "    import shutil\n",
        "    import json\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    print('\\n- Predicting and selecting best model...')\n",
        "    best_model = []; save_settings = []; time.sleep(1); \n",
        "    try: \n",
        "        with open(set_dir+'/processed_predictions.json') as fn: processed = json.load(fn)\n",
        "    except: processed = {}\n",
        "    saved_models_disk = (os.listdir(use_dir))\n",
        "    saved_models_disk_len = len(saved_models_disk)\n",
        "    settings_dict_disk = joblib.load(set_dir+'/settings_dict.pkl')\n",
        "    settings_dict_len = len(settings_dict_disk)\n",
        "    if saved_models_disk_len > settings_dict_len: \n",
        "        smf = [mi for mi in saved_models_disk if mi not in settings_dict_disk]\n",
        "        for remove_mod in smf:\n",
        "            os.remove(use_dir+'/'+remove_mod)\n",
        "    use_dir_ = os.listdir(use_dir)\n",
        "    for mod in tqdm(use_dir_):\n",
        "        if mod not in processed:\n",
        "            if mixed_ts == True:\n",
        "                ts = joblib.load(set_dir+'/settings_dict.pkl')[mod]['time-steps']\n",
        "                with block_pr(block_printing=True):\n",
        "                    start, split, target_pred_cropped, target_pred_uncropped, \\\n",
        "                    _, _, _, _, _, _, _, input_shape, _ = gen_rnn_data(link=link, time_steps=ts, crop_point=crop_point,\n",
        "                                                                        gen_time_series=gen_time_series, \n",
        "                                                                        scaler=False, target_pred=target_pred, \n",
        "                                                                        tr_data_dir=tr_data_dir)\n",
        "                loop_input = target_pred_cropped[split:]\n",
        "            evals = []; print('\\n')\n",
        "            for t1 in range(t2): \n",
        "                pred, r2, r2_ts, r2_all, graph_path, pred_path = predict(model=(use_dir+'/'+mod), loop_input=loop_input, \n",
        "                                                                        use_dir=use_dir, graphs_dir=graphs_dir,\n",
        "                                                                        set_dir=set_dir, pred_dir=pred_dir, \n",
        "                                                                        target_pred_cropped=target_pred_cropped, \n",
        "                                                                        target_pred_uncropped=target_pred_uncropped,\n",
        "                                                                        acc=0.85, ext=ext, time_steps=ts, \n",
        "                                                                        loops=loops, scaler=scaler, thrsh=t1, \n",
        "                                                                        start=start, split=split, \n",
        "                                                                        eval_pr=eval_pr, save_prd=True,\n",
        "                                                                        save_accuracy=save_accuracy)\n",
        "                with block_pr(block_printing):\n",
        "                    print('    - r2: %.2f' % r2+ ' ## r2_time_steps: %.2f' % r2_ts+ ' ## r2_all_duration: %.2f' % r2_all)\n",
        "                update_dict = {'t1':t1, 'r2_time_steps': r2_ts, 'r2_all_duration': r2_all, \n",
        "                                'r2_sum': r2_all+r2_ts, 'graph_path': graph_path, 'prediction_path': pred_path}\n",
        "                json_name = set_dir+'/settings_'+str(t1)+'_'+mod+'.json'\n",
        "                # get settings\n",
        "                mod_settings = get_settings(mod=mod, settings=settings, json_name=json_name, update_dict=update_dict)\n",
        "                with open(json_name, 'w') as fp: json.dump(mod_settings, fp)\n",
        "                evals.append((r2_all))\n",
        "\n",
        "            # select models with certain accuracy\n",
        "            save_settings = select_models_acc(evals=evals, save_accuracy=save_accuracy, set_dir=set_dir, \n",
        "                                              mod=mod, save_settings=save_settings)\n",
        "\n",
        "            # select settings of highest accuracy\n",
        "            best_model, best_settings = select_best_models_acc(evals=evals, mod=mod, \n",
        "                                                               set_dir=set_dir, best_model=best_model)\n",
        "\n",
        "            with block_pr(block_printing):\n",
        "                print('- Best settings-acc for '+mod+' model are: '); \n",
        "                for i in list(best_settings.items())[:-1]: print('     -', i[0], ': ', i[1]); \n",
        "            time.sleep(1)\n",
        "            del best_settings; del r2; del r2_ts; del r2_all; del graph_path; del pred_path\n",
        "            processed[mod] = 1\n",
        "            with open(set_dir+'/processed_predictions.json', 'w') as fp: json.dump(processed, fp)\n",
        "        else:\n",
        "            evals = []\n",
        "            for t1p in range(t2): \n",
        "                json_name = set_dir+'/settings_'+str(t1p)+'_'+mod+'.json'\n",
        "                mod_settings = get_settings(mod=mod, json_name=json_name, get_settings_only=True)\n",
        "                r2_all = mod_settings['r2_all_duration']\n",
        "                evals.append(r2_all)\n",
        "            # select models with certain accuracy\n",
        "            save_settings = select_models_acc(evals=evals, save_accuracy=save_accuracy, set_dir=set_dir, \n",
        "                                              mod=mod, save_settings=save_settings)\n",
        "            # select settings of highest accuracy\n",
        "            best_model, best_settings = select_best_models_acc(evals=evals, mod=mod, \n",
        "                                                               set_dir=set_dir, best_model=best_model)\n",
        "    # save all models in table\n",
        "    selected_models = save_tables(save_settings=save_settings, ext=ext, set_dir=set_dir)\n",
        "    # save metadata\n",
        "    try: prediction_inf = pred\n",
        "    except: \n",
        "        try: pred = pd.read_csv(pred_dir+'/'+os.listdir(pred_dir)[0], index_col=0)\n",
        "        except: pred = None\n",
        "    save_tables(ext=ext, dates_info=True, prediction_inf=pred, time_steps=ts, set_dir=set_dir,\n",
        "                train_end_date=train_end_date, train_start_date=train_start_date)\n",
        "    # save data structure\n",
        "    save_data_structure(selected_models=selected_models, ext=ext, graphs_dir=graphs_dir, \n",
        "                        set_dir=set_dir, pred_dir=pred_dir, use_dir=use_dir)\n",
        "    \n",
        "    # select and save best model\n",
        "    evals = []\n",
        "    for i in best_model: \n",
        "        eval_r = i['r2_all_duration']\n",
        "        evals.append(eval_r)#+i['r2_time_steps'])\n",
        "    b_mod = evals.index(max(evals)); best_model = best_model[b_mod]\n",
        "\n",
        "    print('- Best settings-acc for best model ( '+best_model['model']+' ) are: '); \n",
        "    for i in list(best_model.items())[:]: print('     -', i[0], ': ', i[1]); \n",
        "    time.sleep(1)\n",
        "    model, t1, r2 = best_model['model'], best_model['t1'], best_model['r2_all_duration']\n",
        "    gp, pp = best_model['graph_path'], best_model['prediction_path']\n",
        "    try: plt.imshow(plt.imread(gp), aspect='auto')\n",
        "    except: pass\n",
        "    return model, t1, r2, gp, pp, None\n",
        "\n",
        "def recursive_train_predict(time_steps=10, gen_time_series=False, epochs=300, bs=1*32, vl=0.1, rnn=GRU, tr=False,\n",
        "                            l=[64, 100, 32], dropout=0, convrnn=False, time_steps_rt=10, reconst_retrain=False,\n",
        "                            gen_time_series_rt=False, epochs_rt=300, bs_rt=1*32, vl_rt=0.1, rnn_rt=GRU, \n",
        "                            l_rt=[64, 100, 32], dropout_rt=0, convrnn_rt=False, t2=4, loops=10, loops_rt=10, \n",
        "                            reconst=False, eval_pr=4, eval_pr_rt=7, reconst_loops=2, block_printing=False,\n",
        "                            increase_time_steps=False, seed_input=6, start_prediction=1, ext=10, target_pred=None, \n",
        "                            link=None, save_accuracy=0.7, train_start_date=None, gdrive=None, mixed_ts=False, crop_point=110):\n",
        "    '''\n",
        "    this function is used to automate the whole process of training, predicting, \n",
        "    evaluating, reconstructing the dataset, and retraining\n",
        "    '''\n",
        "    import json\n",
        "    import joblib\n",
        "    from tqdm import tqdm\n",
        "    import time \n",
        "    import shutil\n",
        "    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
        "    from sklearn.naive_bayes import GaussianNB, MultinomialNB, CategoricalNB, BernoulliNB, ComplementNB\n",
        "    from sklearn.manifold import MDS, Isomap\n",
        "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "    from sklearn.decomposition import FastICA, FactorAnalysis, TruncatedSVD, NMF\n",
        "    from datetime import datetime\n",
        "    pd.set_option('display.max_rows', None)\n",
        "\n",
        "    tdm_dir = gdrive+'trained_models'; rtdm_dir = gdrive+'retrained_models'; set_dir = gdrive+'settings'\n",
        "    graphs_dir = gdrive+'graphs'; reconst_dir = gdrive+'reconstructed_datasets'; bm_dir = gdrive+'best_models'\n",
        "    pred_dir = gdrive+'predictions'; set_dir_meta = set_dir+'/metadata'; tr_data_dir = gdrive+'training_data'\n",
        "    if os.path.exists(tdm_dir) == False: os.makedirs(tdm_dir)\n",
        "    if os.path.exists(rtdm_dir) == False: os.makedirs(rtdm_dir)\n",
        "    if os.path.exists(set_dir) == False: os.makedirs(set_dir)\n",
        "    if os.path.exists(set_dir_meta) == False: os.makedirs(set_dir_meta)\n",
        "    if os.path.exists(graphs_dir) == False: os.makedirs(graphs_dir)\n",
        "    if os.path.exists(reconst_dir) == False: os.makedirs(reconst_dir)\n",
        "    if os.path.exists(bm_dir) == False: os.makedirs(bm_dir)\n",
        "    if os.path.exists(pred_dir) == False: os.makedirs(pred_dir)\n",
        "    if os.path.exists(tr_data_dir) == False: os.makedirs(tr_data_dir)\n",
        "\n",
        "    with block_pr(block_printing):\n",
        "        start, split, target_pred_cropped, target_pred_uncropped, \\\n",
        "        X, Y, trgt_x, trgt_y, gen_xx, gen_yy, scaler, input_shape, data = gen_rnn_data(link=link, time_steps=time_steps, \n",
        "                                                                        gen_time_series=gen_time_series, \n",
        "                                                                        scaler=False, target_pred=target_pred, \n",
        "                                                                        tr_data_dir=tr_data_dir, crop_point=crop_point)\n",
        "        \n",
        "    if tr == True:\n",
        "        X, Y = X.reshape(X.shape[0], X.shape[1]), Y.reshape(Y.shape[0], Y.shape[1]), \n",
        "        sc_da = FactorAnalysis().fit(X, Y); \n",
        "        X, Y = sc_da.transform(X), sc_da.transform(Y)\n",
        "        X, Y = (X).reshape(X.shape[0], X.shape[1], 1), (Y).reshape(Y.shape[0], Y.shape[1], 1)\n",
        "        print(X.shape, Y.shape)\n",
        "    ######### TRAIN MODEL ###\n",
        "    if gen_time_series == True: X, Y = gen_xx, gen_yy\n",
        "    with block_pr(block_printing):\n",
        "        model, history, settings_rnn, = train_rnn(X=X, Y=Y, time_steps=time_steps, epochs=epochs, \n",
        "                                                  bs=bs, vl=vl, vd=(trgt_x, trgt_y), rnn=rnn, l=l, \n",
        "                                                  dropout=dropout, convrnn=convrnn, \n",
        "                                                  input_shape=input_shape, tdm_dir=tdm_dir, \n",
        "                                                  rtdm_dir=rtdm_dir, seed_input_tr=seed_input, set_dir=set_dir)\n",
        "    \n",
        "    ######### GET BEST MODEL ###\n",
        "    # if seed_input[1] == start_prediction:\n",
        "    if start_prediction == 0:\n",
        "        del X; del Y; del trgt_x; del trgt_y; del gen_xx; del gen_yy\n",
        "        if rnn != None: train_end_date = str(datetime.date(datetime.now()))\n",
        "        else: \n",
        "            ask_train_date = input('- training end date = '+str(datetime.date(datetime.now()))+' ? ')\n",
        "            if ask_train_date == ('no' or 'n' or 'No' or 'N'):\n",
        "                train_end_date = input('- enter training end date (format: y-m-d: 2020-00-00) = ')\n",
        "            else: train_end_date = str(datetime.date(datetime.now()))\n",
        "        settings_dict = joblib.load(gdrive+'settings/settings_dict.pkl')\n",
        "        loop_input = target_pred_cropped[split:]\n",
        "        model, t1, r2, graph_path, prediction_path, fig_st = predict_select_model(mixed_ts=mixed_ts,\n",
        "                                                             link=link, gen_time_series=gen_time_series, \n",
        "                                                             target_pred=target_pred, tr_data_dir=tr_data_dir,\n",
        "                                                             loop_input=loop_input, \n",
        "                                                             use_dir=tdm_dir, set_dir=set_dir, bm_dir=bm_dir,\n",
        "                                                             settings=settings_dict, graphs_dir=graphs_dir,\n",
        "                                                             ts=time_steps, rnn=rnn, eval_pr=eval_pr, pred_dir=pred_dir,\n",
        "                                                             target_pred_cropped=target_pred_cropped, \n",
        "                                                             target_pred_uncropped=target_pred_uncropped, ext=ext,\n",
        "                                                             loops=loops, t2=t2, scaler=False, start=start, \n",
        "                                                             split=split, block_printing=block_printing, \n",
        "                                                             save_accuracy=save_accuracy, \n",
        "                                                             train_end_date=train_end_date,\n",
        "                                                             train_start_date=train_start_date, crop_point=crop_point)\n",
        "    \n",
        "\n",
        "def train_ml(X=None, Y=None, scaler=0, print_eval=False, use_model='dtree', cv=False, \n",
        "             acc=0.5, ts=0.5, reduce_acc=True):\n",
        "    '''this function is used to train models (adversarial evaluation models)'''\n",
        "    import joblib\n",
        "    from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "    from sklearn import svm\n",
        "    from sklearn.isotonic import IsotonicRegression\n",
        "    from sklearn.linear_model import LinearRegression, LassoLars, ARDRegression, HuberRegressor, LogisticRegression\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    import xgboost as xgb\n",
        "    from sklearn import tree\n",
        "    from sklearn.model_selection import train_test_split, KFold, cross_val_score \n",
        "    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "    from warnings import simplefilter\n",
        "    simplefilter(action='ignore', category=FutureWarning)\n",
        "    c = 0\n",
        "    stop = False\n",
        "    while stop != True:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=ts) \n",
        "        if scaler == 1: \n",
        "            scaler = RobustScaler().fit(X_train); \n",
        "            X_train = scaler.transform(X_train); X_test = scaler.transform(X_test)\n",
        "        models = {'huber':HuberRegressor(), 'lr':LinearRegression(), '0': RandomForestRegressor(),\n",
        "                  '1':xgb.XGBRegressor(objective='reg:squarederror'), '2': tree.DecisionTreeRegressor(), \n",
        "                  '3': LogisticRegression()} \n",
        "        model = models[use_model]; model = model.fit((X_train), (y_train))\n",
        "\n",
        "        score = model.score(X_test, y_test) \n",
        "        y_predicted = model.predict(X_test)#np.array([int(i) for i in (model1.predict(X_test))])\n",
        "        mse = mean_squared_error(y_test, y_predicted, squared=False)\n",
        "        mae = mean_absolute_error(y_test, y_predicted)\n",
        "        r2 = r2_score(y_test, y_predicted)\n",
        "        kf = KFold(n_splits=4, shuffle=True)\n",
        "        if cv == True: cv = cross_val_score(model, (X_test), (y_test), cv=kf).mean()\n",
        "        if score > 0.0: print('score         : ', score)\n",
        "        if score > acc:\n",
        "            stop = True; #joblib.dump(model, 'model_.pkl')\n",
        "            if print_eval == True:\n",
        "                print('score         : ', score)\n",
        "                print('mse               : ', mse)\n",
        "                print('mae               : ', mae)\n",
        "                # print('cv                 : ', cv)\n",
        "                # print('r2               : ', r2)\n",
        "        c+=1\n",
        "        if c == 30: c = 0; \n",
        "        if reduce_acc == True: acc -= 0.05\n",
        "    return model, scaler, acc\n",
        "\n",
        "def adversarial_evaluation(prediction=None, acc=0.85):\n",
        "    '''\n",
        "    this function is used to train and concatenate adversarial evaluation model and outputs\n",
        "    data structure:\n",
        "    x = true input; y = false output, z = true output, f = false_input\n",
        "    - function:  input = y >>>>>>> evaluate to >>>>>>> output = z\n",
        "    '''\n",
        "    prediction_values = prediction.dropna().to_numpy()\n",
        "    false_values_all = prediction[1].dropna().to_numpy().reshape(-1, 1)\n",
        "    false_values = prediction_values[20:,1].reshape(-1, 1) #[20:,1]\n",
        "    true_values = prediction_values[20:,2]; true_values.reshape(true_values.shape[0], ) #[20:,1]\n",
        "    # train / predict\n",
        "    model, scaler, true_acc = train_ml(X=false_values, Y=true_values, scaler=0, print_eval=True, use_model='lr', acc=acc)\n",
        "    evaluated_values = model.predict(false_values_all)\n",
        "    np.set_printoptions(suppress=True,formatter={'float_kind':'{:16.3f}'.format}, linewidth=130)\n",
        "    evaluated_values = [int(i) for i in evaluated_values]\n",
        "    evaluated_values = pd.DataFrame(evaluated_values).rename(columns={0:'evaluated prediction (acc:%.2f' % (true_acc)+')'}).reset_index(drop=True)\n",
        "    prediction_evaluated = pd.concat((prediction, evaluated_values), \n",
        "                                     axis=1).rename(columns={0:'date', 1:'model prediction', 2:'true values'})\n",
        "    prediction_evaluated = prediction_evaluated[['date', 'model prediction', 'evaluated prediction (acc:%.2f' % (true_acc)+')', 'true values']]\n",
        "    return prediction_evaluated[:len(prediction_evaluated['evaluated prediction (acc:%.2f' % (true_acc)+')'].dropna())], true_acc\n",
        "\n",
        "def random_seed_changer(seed_value_input=0, sv2=1):\n",
        "    \"\"\"\n",
        "    - The random seed changer function allows deterministic settings during the training session\n",
        "    - In order to use non-deterministic settings, the comments below should be enabled to ensure that a GPU device is selected.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    import random\n",
        "    # os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    os.environ['PYTHONHASHSEED']=str(seed_value_input)\n",
        "    np.random.seed(seed_value_input)\n",
        "    random.seed(seed_value_input)\n",
        "    tf.random.set_seed(seed_value_input+sv2)\n",
        "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "    tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-1CwS7q1UJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recursive_train_predict function\n",
        "##########\n",
        "# 15 time-steps (total of 30 X+Y)\n",
        "# Deterministic - Train on CPU\n",
        "##########\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "target_pred_x = input('- Enter target_pred: ')\n",
        "logging.disable(logging.WARNING) \n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "\n",
        "# get training start date\n",
        "ask_train_date = input('- training start date = '+str(datetime.date(datetime.now()))+' ?')\n",
        "if ask_train_date == ('no' or 'n' or 'No' or 'N'):\n",
        "    train_start_date = input('- enter training start date (format: y-m-d: 2020-00-00) = ')\n",
        "else: train_start_date = str(datetime.date(datetime.now()))\n",
        "\n",
        "# use google drive for storage\n",
        "root_fldr_name = 15\n",
        "use_gdrive = False\n",
        "###################\n",
        "if use_gdrive == True:\n",
        "    from google.colab import drive\n",
        "    root_fldr = 'Data from Colab_'+str(root_fldr_name)\n",
        "    drive.mount('/content/drive')\n",
        "    folders_list = [root_fldr, root_fldr+'/trained_models', root_fldr+'/settings',\n",
        "                    root_fldr+'/settings/metadata', root_fldr+'/graphs', root_fldr+'/predictions',\n",
        "                    root_fldr+'/training_data']\n",
        "    for fldr in folders_list:\n",
        "        try: os.makedirs('drive/My Drive/'+fldr)\n",
        "        except: pass\n",
        "    gdrive = 'drive/My Drive/'+root_fldr+'/'\n",
        "else: \n",
        "    gdrive = ''\n",
        "    try: os.makedirs('training_data')\n",
        "    except:pass\n",
        "    try: os.makedirs('settings')\n",
        "    except:pass\n",
        "\n",
        "# link to training data\n",
        "link = 'local'\n",
        "root_fn = 'data_input.csv'\n",
        "try:\n",
        "    link = gdrive+'training_data/'+root_fn\n",
        "    shutil.copy2(root_fn, gdrive+'training_data/')\n",
        "except:\n",
        "    link = input('- Enter link: ')\n",
        "    urllib.request.urlretrieve(link, gdrive+'training_data/'+root_fn)\n",
        "\n",
        "# initial settings\n",
        "crop_point = 110\n",
        "t2 = 15\n",
        "try: r1, r2 = joblib.load(gdrive+'settings/random_seeds.pkl') \n",
        "except: \n",
        "    r1 = random.randint(0,9999999); r2 = random.randint(0,9999999)\n",
        "    joblib.dump([r1, r2], gdrive+'settings/random_seeds.pkl')\n",
        "seeds_list_selected = [r1,30,18,1,35,21,43,31,47,46,9,3,17,42,29,25,41]\n",
        "tf_seeds_list = [r2,0,43,8,7,17,2,27,19,9195911] \n",
        "sl = True\n",
        "start_seed = 0\n",
        "end_seed = 2\n",
        "###################\n",
        "if sl == True: seeds_list = seeds_list_selected\n",
        "else: seeds_list = [s for s in range(start_seed,end_seed)]\n",
        "number_of_loops = len(seeds_list)*len(tf_seeds_list)\n",
        "\n",
        "# variations - deterministic\n",
        "\n",
        "time_steps = [15] # number of initial time steps to predict \n",
        "vl_list = [0.3]\n",
        "epochs_list = [300,600,1000]\n",
        "bs_list = [1024]\n",
        "l_list = [[16, 32, 16], [32, 64, 32], [32, 16, 32], [64, 32, 64]]\n",
        "\n",
        "strt = len(vl_list)*len(epochs_list)*len(bs_list)*len(l_list)*len(time_steps)\n",
        "start_prediction = strt * (number_of_loops)# -1\n",
        "print(start_prediction)\n",
        "\n",
        "# actually train and predict\n",
        "seed_source = seeds_list\n",
        "\n",
        "for i_seed in tqdm(seed_source): \n",
        "    for sv2 in tf_seeds_list:\n",
        "        random_seed_changer(seed_value_input=i_seed, sv2=sv2)\n",
        "        used_seed = [i_seed, sv2]\n",
        "        for tss in time_steps:\n",
        "            for vl in tqdm(vl_list):\n",
        "                for epochs in tqdm(epochs_list):\n",
        "                    for bs in bs_list:\n",
        "                        for l in l_list:\n",
        "                            start_prediction -= 1\n",
        "                            print(start_prediction, i_seed)\n",
        "                            recursive_train_predict(target_pred=target_pred_x, link=link, mixed_ts=False, crop_point=crop_point,\n",
        "                                                    time_steps=tss, gen_time_series=False, epochs=epochs, bs=bs, \n",
        "                                                    vl=vl, l=l, dropout=0, convrnn=True, \n",
        "                                                    time_steps_rt=10, gen_time_series_rt=False, epochs_rt=600, bs_rt=1*32, \n",
        "                                                    vl_rt=0.1, l_rt=[300, 100, 32], dropout_rt=0, convrnn_rt=True, \n",
        "                                                    rnn=GRU, #train gru or lstm\n",
        "                                                    rnn_rt=None, #retrain gru or lstm\n",
        "                                                    tr=False,\n",
        "                                                    t2= t2, # number of generated predictions to keep at every prediction loop\n",
        "                                                    loops='auto', # loops for prediction\n",
        "                                                    reconst=True, #reconstruct the dataset \n",
        "                                                    eval_pr=4, eval_pr_rt=5, # beside the accuracy of total duration, check the accuracy of first n time-steps\n",
        "                                                    reconst_loops=2, # how many reconstruction loops\n",
        "                                                    increase_time_steps=False, # increase time-steps during reconstruction of datasets\n",
        "                                                    reconst_retrain=False, \n",
        "                                                    seed_input=used_seed, # seed for the training process\n",
        "                                                    start_prediction=start_prediction, # start prediction in a specific loop\n",
        "                                                    ext='2020-10-01', # end date for prediction\n",
        "                                                    block_printing=True, # block printing \n",
        "                                                    save_accuracy= 0.0, # accuracy threshold for saving data\n",
        "                                                    train_start_date=train_start_date, # start date for training\n",
        "                                                    gdrive=gdrive) # save to google drive\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcNFHRih1i1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recursive_train_predict function\n",
        "##########\n",
        "# 20 time-steps (total of 40 X+Y)\n",
        "# Deterministic - Train on CPU\n",
        "##########\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "target_pred_x = input('- Enter target_pred: ')\n",
        "logging.disable(logging.WARNING) \n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "\n",
        "# get training start date\n",
        "ask_train_date = input('- training start date = '+str(datetime.date(datetime.now()))+' ?')\n",
        "if ask_train_date == ('no' or 'n' or 'No' or 'N'):\n",
        "    train_start_date = input('- enter training start date (format: y-m-d: 2020-00-00) = ')\n",
        "else: train_start_date = str(datetime.date(datetime.now()))\n",
        "\n",
        "# use google drive for storage\n",
        "root_fldr_name = 20\n",
        "use_gdrive = False\n",
        "###################\n",
        "if use_gdrive == True:\n",
        "    from google.colab import drive\n",
        "    root_fldr = 'Data from Colab_'+str(root_fldr_name)\n",
        "    drive.mount('/content/drive')\n",
        "    folders_list = [root_fldr, root_fldr+'/trained_models', root_fldr+'/settings',\n",
        "                    root_fldr+'/settings/metadata', root_fldr+'/graphs', root_fldr+'/predictions',\n",
        "                    root_fldr+'/training_data']\n",
        "    for fldr in folders_list:\n",
        "        try: os.makedirs('drive/My Drive/'+fldr)\n",
        "        except: pass\n",
        "    gdrive = 'drive/My Drive/'+root_fldr+'/'\n",
        "else: \n",
        "    gdrive = ''\n",
        "    try:os.makedirs('training_data')\n",
        "    except:pass\n",
        "    try: os.makedirs('settings')\n",
        "    except:pass\n",
        "\n",
        "# link to training data\n",
        "root_fn = 'data_input.csv'\n",
        "try:\n",
        "    link = gdrive+'training_data/'+root_fn\n",
        "    shutil.copy2(root_fn, gdrive+'training_data/')\n",
        "except:\n",
        "    link = input('- Enter link: ')\n",
        "    urllib.request.urlretrieve(link, gdrive+'training_data/'+root_fn)\n",
        "\n",
        "# initial settings\n",
        "crop_point = 110\n",
        "t2 = 20\n",
        "try: r1, r2 = joblib.load(gdrive+'settings/random_seeds.pkl') \n",
        "except: \n",
        "    r1 = random.randint(0,9999999); r2 = random.randint(0,9999999)\n",
        "    joblib.dump([r1, r2], gdrive+'settings/random_seeds.pkl')\n",
        "seeds_list_selected = [r1,30,18,1,35,21,43,31,47,46,9,3,17,42,29,25,41]\n",
        "tf_seeds_list = [r2,0,43,8,7,17,2,27,19,9195911] \n",
        "sl = True\n",
        "start_seed = 0\n",
        "end_seed = 2\n",
        "###################\n",
        "if sl == True: seeds_list = seeds_list_selected\n",
        "else: seeds_list = [s for s in range(start_seed,end_seed)]\n",
        "number_of_loops = len(seeds_list)*len(tf_seeds_list)\n",
        "\n",
        "# variations - deterministic\n",
        "\n",
        "time_steps = [20] # number of initial time steps to predict \n",
        "vl_list = [0.3]\n",
        "epochs_list = [300,600,1000,1200]\n",
        "bs_list = [1024]\n",
        "l_list = [[16, 32, 16], [32, 64, 32], [32, 16, 32], [64, 32, 64]]\n",
        "\n",
        "strt = len(vl_list)*len(epochs_list)*len(bs_list)*len(l_list)*len(time_steps)\n",
        "start_prediction = strt * (number_of_loops)# -1\n",
        "print(start_prediction)\n",
        "\n",
        "# actually train and predict\n",
        "seed_source = seeds_list\n",
        "\n",
        "for i_seed in tqdm(seed_source): \n",
        "    for sv2 in tf_seeds_list:\n",
        "        random_seed_changer(seed_value_input=i_seed, sv2=sv2)\n",
        "        used_seed = [i_seed, sv2]\n",
        "        for tss in time_steps:\n",
        "            for vl in tqdm(vl_list):\n",
        "                for epochs in tqdm(epochs_list):\n",
        "                    for bs in bs_list:\n",
        "                        for l in l_list:\n",
        "                            start_prediction -= 1\n",
        "                            print(start_prediction, i_seed)\n",
        "                            recursive_train_predict(target_pred=target_pred_x, link=link, mixed_ts=False, crop_point=crop_point,\n",
        "                                                    time_steps=tss, gen_time_series=False, epochs=epochs, bs=bs, \n",
        "                                                    vl=vl, l=l, dropout=0, convrnn=True, \n",
        "                                                    time_steps_rt=10, gen_time_series_rt=False, epochs_rt=600, bs_rt=1*32, \n",
        "                                                    vl_rt=0.1, l_rt=[300, 100, 32], dropout_rt=0, convrnn_rt=True, \n",
        "                                                    rnn=GRU, #train gru or lstm\n",
        "                                                    rnn_rt=None, #retrain gru or lstm\n",
        "                                                    tr=False,\n",
        "                                                    t2= t2, # number of generated predictions to keep at every prediction loop\n",
        "                                                    loops='auto', # loops for prediction\n",
        "                                                    reconst=True, #reconstruct the dataset \n",
        "                                                    eval_pr=4, eval_pr_rt=5, # beside the accuracy of total duration, check the accuracy of first n time-steps\n",
        "                                                    reconst_loops=2, # how many reconstruction loops\n",
        "                                                    increase_time_steps=False, # increase time-steps during reconstruction of datasets\n",
        "                                                    reconst_retrain=False, \n",
        "                                                    seed_input=used_seed, # seed for the training process\n",
        "                                                    start_prediction=start_prediction, # start prediction in a specific loop\n",
        "                                                    ext='2020-10-01', # end date for prediction\n",
        "                                                    block_printing=True, # block printing \n",
        "                                                    save_accuracy= 0.0, # accuracy threshold for saving data\n",
        "                                                    train_start_date=train_start_date, # start date for training\n",
        "                                                    gdrive=gdrive) # save to google drive\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZYZuXUgq3B_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recursive_train_predict function\n",
        "##########\n",
        "# 15 time-steps (total of 30 X+Y)\n",
        "# Non-deterministic - Train on GPU\n",
        "##########\n",
        "\n",
        "# Recursive_train_predict function\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "target_pred_x = input('- Enter target_pred: ')\n",
        "logging.disable(logging.WARNING) \n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "\n",
        "\n",
        "# get training start date\n",
        "ask_train_date = input('- training start date = '+str(datetime.date(datetime.now()))+' ?')\n",
        "if ask_train_date == ('no' or 'n' or 'No' or 'N'):\n",
        "    train_start_date = input('- enter training start date (format: y-m-d: 2020-00-00) = ')\n",
        "else: train_start_date = str(datetime.date(datetime.now()))\n",
        "\n",
        "# use google drive for storage\n",
        "root_fldr_name = '15-Non-deterministic'\n",
        "use_gdrive = False\n",
        "###################\n",
        "if use_gdrive == True:\n",
        "    from google.colab import drive\n",
        "    root_fldr = 'Data from Colab_'+str(root_fldr_name)\n",
        "    drive.mount('/content/drive')\n",
        "    folders_list = [root_fldr, root_fldr+'/trained_models', root_fldr+'/settings',\n",
        "                    root_fldr+'/settings/metadata', root_fldr+'/graphs', root_fldr+'/predictions',\n",
        "                    root_fldr+'/training_data']\n",
        "    for fldr in folders_list:\n",
        "        try: os.makedirs('drive/My Drive/'+fldr)\n",
        "        except: pass\n",
        "    gdrive = 'drive/My Drive/'+root_fldr+'/'\n",
        "else: \n",
        "    gdrive = ''\n",
        "    try:os.makedirs('training_data')\n",
        "    except:pass\n",
        "    try:os.makedirs('settings')\n",
        "    except:pass\n",
        "\n",
        "# link to training data\n",
        "root_fn = 'data_input.csv'\n",
        "try:\n",
        "    link = gdrive+'training_data/'+root_fn\n",
        "    shutil.copy2(root_fn, gdrive+'training_data/')\n",
        "except:\n",
        "    link = input('- Enter link: ')\n",
        "    urllib.request.urlretrieve(link, gdrive+'training_data/'+root_fn)\n",
        "\n",
        "# initial settings\n",
        "crop_point = 110\n",
        "t2 = 15\n",
        "try: r1, r2 = joblib.load(gdrive+'settings/random_seeds.pkl') \n",
        "except: \n",
        "    r1 = random.randint(0,9999999); r2 = random.randint(0,9999999)\n",
        "    joblib.dump([r1, r2], gdrive+'settings/random_seeds.pkl')\n",
        "seeds_list_selected = [13]\n",
        "tf_seeds_list = [4] #9195911\n",
        "sl = True\n",
        "start_seed = 0\n",
        "end_seed = 2\n",
        "###################\n",
        "if sl == True: seeds_list = seeds_list_selected\n",
        "else: seeds_list = [s for s in range(start_seed,end_seed)]\n",
        "number_of_loops = len(seeds_list)*len(tf_seeds_list)\n",
        "\n",
        "# variations - deterministic\n",
        "\n",
        "time_steps = [15] # number of initial time steps to predict \n",
        "vl_list = [0.3]\n",
        "epochs_list = [300,600]\n",
        "bs_list = [1024]\n",
        "l_list = [[16, 32, 16], [32, 64, 32], [64, 128, 64], [128, 256, 128], \n",
        "          [32, 16, 32], [64, 32, 64], [128, 64, 128], [256, 128, 256]]\n",
        "\n",
        "strt = len(vl_list)*len(epochs_list)*len(bs_list)*len(l_list)*len(time_steps)\n",
        "start_prediction = strt * (number_of_loops)# -1\n",
        "print(start_prediction)\n",
        "\n",
        "# actually train and predict #784\n",
        "seed_source = seeds_list\n",
        "\n",
        "for i_seed in tqdm(seed_source): \n",
        "    for sv2 in tf_seeds_list:\n",
        "        random_seed_changer(seed_value_input=i_seed, sv2=sv2)\n",
        "        used_seed = [i_seed, sv2]\n",
        "        for tss in time_steps:\n",
        "            for vl in tqdm(vl_list):\n",
        "                for epochs in tqdm(epochs_list):\n",
        "                    for bs in bs_list:\n",
        "                        for l in l_list:\n",
        "                            start_prediction -= 1\n",
        "                            print(start_prediction, i_seed)\n",
        "                            # os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "                            recursive_train_predict(target_pred=target_pred_x, link=link, mixed_ts=False, crop_point=crop_point,\n",
        "                                                    time_steps=tss, gen_time_series=False, epochs=epochs, bs=bs, \n",
        "                                                    vl=vl, l=l, dropout=0, convrnn=True, \n",
        "                                                    time_steps_rt=10, gen_time_series_rt=False, epochs_rt=600, bs_rt=1*32, \n",
        "                                                    vl_rt=0.1, l_rt=[300, 100, 32], dropout_rt=0, convrnn_rt=True, \n",
        "                                                    rnn=GRU, #train gru or lstm\n",
        "                                                    rnn_rt=None, #retrain gru or lstm\n",
        "                                                    tr=False,\n",
        "                                                    t2= t2, # number of generated predictions to keep at every prediction loop\n",
        "                                                    loops='auto', # loops for prediction\n",
        "                                                    reconst=True, #reconstruct the dataset \n",
        "                                                    eval_pr=4, eval_pr_rt=5, # beside the accuracy of total duration, check the accuracy of first n time-steps\n",
        "                                                    reconst_loops=2, # how many reconstruction loops\n",
        "                                                    increase_time_steps=False, # increase time-steps during reconstruction of datasets\n",
        "                                                    reconst_retrain=False, \n",
        "                                                    seed_input=used_seed, # seed for the training process\n",
        "                                                    start_prediction=start_prediction, # start prediction in a specific loop\n",
        "                                                    ext='2020-10-01', # end date for prediction\n",
        "                                                    block_printing=True, # block printing \n",
        "                                                    save_accuracy= 0.0, # accuracy threshold for saving data\n",
        "                                                    train_start_date=train_start_date, # start date for training\n",
        "                                                    gdrive=gdrive) # save to google drive\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_S2QyEv5MIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recursive_train_predict function\n",
        "##########\n",
        "# 15 time-steps (total of 30 X+Y) - for Technical validation\n",
        "# Non-deterministic - Train on GPU\n",
        "##########\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "target_pred_x = input('- Enter target_pred: ')\n",
        "logging.disable(logging.WARNING) \n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "\n",
        "# get training start date\n",
        "ask_train_date = input('- training start date = '+str(datetime.date(datetime.now()))+' ?')\n",
        "if ask_train_date == ('no' or 'n' or 'No' or 'N'):\n",
        "    train_start_date = input('- enter training start date (format: y-m-d: 2020-00-00) = ')\n",
        "else: train_start_date = str(datetime.date(datetime.now()))\n",
        "\n",
        "# use google drive for storage\n",
        "root_fldr_name = 15\n",
        "use_gdrive = False\n",
        "###################\n",
        "if use_gdrive == True:\n",
        "    from google.colab import drive\n",
        "    root_fldr = 'Data from Colab_'+str(root_fldr_name)\n",
        "    drive.mount('/content/drive')\n",
        "    folders_list = [root_fldr, root_fldr+'/trained_models', root_fldr+'/settings',\n",
        "                    root_fldr+'/settings/metadata', root_fldr+'/graphs', root_fldr+'/predictions',\n",
        "                    root_fldr+'/training_data']\n",
        "    for fldr in folders_list:\n",
        "        try: os.makedirs('drive/My Drive/'+fldr)\n",
        "        except: pass\n",
        "    gdrive = 'drive/My Drive/'+root_fldr+'/'\n",
        "else: \n",
        "    gdrive = ''\n",
        "    try:os.makedirs('training_data')\n",
        "    except:pass\n",
        "    try: os.makedirs('settings')\n",
        "    except:pass\n",
        "\n",
        "# link to training data\n",
        "root_fn = 'data_input.csv'\n",
        "try:\n",
        "    link = gdrive+'training_data/'+root_fn\n",
        "    shutil.copy2(root_fn, gdrive+'training_data/')\n",
        "except:\n",
        "    link = input('- Enter link: ')\n",
        "    urllib.request.urlretrieve(link, gdrive+'training_data/'+root_fn)\n",
        "\n",
        "# initial settings\n",
        "crop_point = 80\n",
        "t2 = 15\n",
        "try: r1, r2 = joblib.load(gdrive+'settings/random_seeds.pkl') \n",
        "except: \n",
        "    r1 = random.randint(0,9999999); r2 = random.randint(0,9999999)\n",
        "    joblib.dump([r1, r2], gdrive+'settings/random_seeds.pkl')\n",
        "seeds_list_selected = [269]\n",
        "tf_seeds_list = [0] \n",
        "sl = False\n",
        "start_seed = 0\n",
        "end_seed = 1001\n",
        "###################\n",
        "if sl == True: seeds_list = seeds_list_selected\n",
        "else: seeds_list = [s for s in range(start_seed,end_seed)]\n",
        "number_of_loops = len(seeds_list)*len(tf_seeds_list)\n",
        "\n",
        "# variations - deterministic\n",
        "\n",
        "time_steps = [15] # number of initial time steps to predict \n",
        "vl_list = [0.2]\n",
        "epochs_list = [300]\n",
        "bs_list = [1024]\n",
        "l_list = [[50, 100, 50]]\n",
        "\n",
        "strt = len(vl_list)*len(epochs_list)*len(bs_list)*len(l_list)*len(time_steps)\n",
        "start_prediction = strt * (number_of_loops)# -1\n",
        "print(start_prediction)\n",
        "\n",
        "# actually train and predict\n",
        "seed_source = seeds_list\n",
        "\n",
        "for i_seed in tqdm(seed_source): \n",
        "    for sv2 in tf_seeds_list:\n",
        "        random_seed_changer(seed_value_input=i_seed, sv2=sv2)\n",
        "        used_seed = [i_seed, sv2]\n",
        "        for tss in time_steps:\n",
        "            for vl in tqdm(vl_list):\n",
        "                for epochs in tqdm(epochs_list):\n",
        "                    for bs in bs_list:\n",
        "                        for l in l_list:\n",
        "                            start_prediction -= 1\n",
        "                            print(start_prediction, i_seed)\n",
        "                            recursive_train_predict(target_pred=target_pred_x, link=link, mixed_ts=False, crop_point=crop_point,\n",
        "                                                    time_steps=tss, gen_time_series=False, epochs=epochs, bs=bs, \n",
        "                                                    vl=vl, l=l, dropout=0, convrnn=True, \n",
        "                                                    time_steps_rt=10, gen_time_series_rt=False, epochs_rt=600, bs_rt=1*32, \n",
        "                                                    vl_rt=0.1, l_rt=[300, 100, 32], dropout_rt=0, convrnn_rt=True, \n",
        "                                                    rnn=GRU, #train gru or lstm\n",
        "                                                    rnn_rt=None, #retrain gru or lstm\n",
        "                                                    tr=False,\n",
        "                                                    t2= t2, # number of generated predictions to keep at every prediction loop\n",
        "                                                    loops='auto', # loops for prediction\n",
        "                                                    reconst=True, #reconstruct the dataset \n",
        "                                                    eval_pr=4, eval_pr_rt=5, # beside the accuracy of total duration, check the accuracy of first n time-steps\n",
        "                                                    reconst_loops=2, # how many reconstruction loops\n",
        "                                                    increase_time_steps=False, # increase time-steps during reconstruction of datasets\n",
        "                                                    reconst_retrain=False, \n",
        "                                                    seed_input=used_seed, # seed for the training process\n",
        "                                                    start_prediction=start_prediction, # start prediction in a specific loop\n",
        "                                                    ext='2020-10-01', # end date for prediction\n",
        "                                                    block_printing=True, # block printing \n",
        "                                                    save_accuracy= -100.0, # accuracy threshold for saving data\n",
        "                                                    train_start_date=train_start_date, # start date for training\n",
        "                                                    gdrive=gdrive) # save to google drive\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axNCVTWvnsuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "- Getting statistical significance \n",
        "'''\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json \n",
        "import joblib\n",
        "\n",
        "##### change set_dir as needed\n",
        "set_dir = '/content/drive/My Drive/Data from Colab_20/settings'\n",
        "#####\n",
        "\n",
        "evals = []; save_accuracy=0.7; t2=15; id50 = []; id60 = []; id70 = []; id80 = []; evalsid = []; e50=[]; e60=[]; e70=[]; e80=[]\n",
        "e0 = []\n",
        "try: \n",
        "    pre_analysis = joblib.load(set_dir+'/analysis.pkl')\n",
        "    evals, evalsid, id40, id50, id60, id70, id80, e0, ee0, e40, e50, e60, e70, e80 = pre_analysis \n",
        "except:\n",
        "    for mod in tqdm(os.listdir(set_dir)):\n",
        "        if (mod != 'metadata') and (mod != 'settings_dict.pkl') and (mod != 'processed_predictions.json') and (mod != 'random_seeds.pkl'):\n",
        "            try:\n",
        "                with open(set_dir+'/'+mod) as fn: \n",
        "                    mod_settings = json.load(fn)\n",
        "            except: print(mod)\n",
        "            try:\n",
        "                r2_all = mod_settings['r2_all_duration']\n",
        "                evals.append(r2_all)\n",
        "                evalsid.append(mod_settings['model'])\n",
        "                if 0.5 > r2_all > 0.4: id40.append(mod_settings['model']); e40.append(r2_all)\n",
        "                if 0.6 > r2_all > 0.5: id50.append(mod_settings['model']); e50.append(r2_all)\n",
        "                if 0.7 > r2_all > 0.6: id60.append(mod_settings['model']); e60.append(r2_all)\n",
        "                if 0.8 > r2_all > 0.7: id70.append(mod_settings['model']); e70.append(r2_all)\n",
        "                if r2_all > 0.8: id80.append(mod_settings['model']); e80.append(r2_all)\n",
        "                if 0 < r2_all < 0.4: e0.append(mod_settings['model']); ee0.append(r2_all)\n",
        "            except: print(mod_settings)\n",
        "    pre_analysis = [evals, evalsid, id40, id50, id60, id70, id80, e0, ee0, e40, e50, e60, e70, e80]\n",
        "    joblib.dump(pre_analysis, set_dir+'/analysis.pkl')\n",
        "\n",
        "e00 = len(np.unique(e0))\n",
        "for i in (np.unique(e0)):\n",
        "    if (i in id40): e00-=1\n",
        "    elif (i in id50): e00-=1  \n",
        "    elif (i in id60): e00-=1\n",
        "    elif (i in id70): e00-=1\n",
        "    elif (i in id80): e00-=1\n",
        "ee = ee0+e40+e50+e60+e70+e80\n",
        "estd = np.std(ee)\n",
        "emean = np.mean(ee)\n",
        "e_80 = len([i for i in evals if i > 0.8])\n",
        "e_70 = len([i for i in evals if 0.8 > i > 0.7])\n",
        "e_60 = len([i for i in evals if 0.7 > i > 0.6])\n",
        "e_50 = len([i for i in evals if 0.6 > i > 0.5])\n",
        "e_40 = len([i for i in evals if 0.5 > i > 0.4])\n",
        "idu_80 = len(np.unique(id80))\n",
        "idu_70 = len(np.unique(id70))\n",
        "idu_60 = len(np.unique(id60))\n",
        "idu_50 = len(np.unique(id50))\n",
        "idu_40 = len(np.unique(id40))\n",
        "idu_0 = len(np.unique(e0))\n",
        "ids = id40+id50+id60+id70+id80\n",
        "evalsidcount = len(np.unique(evalsid))\n",
        "#'acc > 0.8':e_80, 'acc > 0.7':e_70, 'acc > 0.6':e_60, 'acc > 0.5':e_50, 'acc > 0.4':e_40,\n",
        "pr = {'mean':emean, 'standard deviation':estd, \n",
        "      'acc_num_mod > 0.8':idu_80, 'acc_num_mod > 0.7':idu_70, 'acc_num_mod > 0.6':idu_60, 'acc_num_mod > 0.5':idu_50, 'acc_num_mod > 0.4':idu_40, \n",
        "      'acc_num_mod < 0.4':idu_0, 'processed models':evalsidcount,\n",
        "      'len of unique models below threshold': e00,\n",
        "      'len of unique models above threshold': len(np.unique(ids))}\n",
        "\n",
        "for k,v in pr.items():\n",
        "    print(k, ' : ', v)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTH9j0aY2xzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Quick testing for models\n",
        "\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "target_pred_x = input('- Enter target_pred: ')\n",
        "# load reconstructed dataset\n",
        "# c = joblib.load(l1); \n",
        "time_steps = 15\n",
        "crop_point = 110\n",
        "\n",
        "link = input('- Enter link: ')\n",
        "\n",
        "# gen time-series data\n",
        "# add the c variable above if enabled in retrain parameter in gen_rnn_data instead of False \n",
        "start, split, target_pred_cropped, target_pred_uncropped, \\\n",
        "        _, _, _, _, _, _, _, _, _ = gen_rnn_data(link=link, time_steps=time_steps, tr_data_dir='training_data', crop_point=crop_point,\n",
        "                                                 gen_time_series=False, retrain=False, target_pred=target_pred_x)\n",
        "\n",
        "# set input to the model\n",
        "loop_input = target_pred_cropped[split:]\n",
        "X, Y = target_pred_cropped[start:split], target_pred_cropped[split:split+time_steps]\n",
        "\n",
        "# try different settings\n",
        "for mod in os.listdir('trained_models'):\n",
        "    for i in range(15):\n",
        "        predications, r2, r2_ts, r2_all, graph_path, pred_path = predict(model=('trained_models/'+mod), loop_input=loop_input, \n",
        "                                                                        use_dir='trained_models', graphs_dir='graphs',\n",
        "                                                                        set_dir='settings', pred_dir='predictions', \n",
        "                                                                        target_pred_cropped=target_pred_cropped, \n",
        "                                                                        target_pred_uncropped=target_pred_uncropped,\n",
        "                                                                        acc=0.85, ext='2020-10-01', time_steps=time_steps, \n",
        "                                                                        loops=220, scaler=False, thrsh=i, \n",
        "                                                                        start=start, split=split, \n",
        "                                                                        eval_pr=4, save_prd=True,\n",
        "                                                                        save_accuracy=-1000)\n",
        "        print('######   ', i, '######')\n",
        "        try:\n",
        "            ff = plt.imread(graph_path)\n",
        "            plt.imshow(ff, aspect='auto')\n",
        "            plt.show()\n",
        "        except: pass\n",
        "        print(r2_ts, r2_all)\n",
        "        print('####################')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}